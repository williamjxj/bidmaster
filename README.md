# ğŸ¯ BidMaster: Your Ultimate Project Bidding Hub

production URL: https://bidmaster-hub.vercel.app/

Tired of juggling multiple freelance platforms, missing out on opportunities, and struggling to track your bids? **BidMaster** is here to change that. We provide a centralized platform to discover, manage, and win more projects, effortlessly.

## âœ¨ Key Features

*   **Discover Opportunities:** We aggregate freelance projects from top platforms like Upwork, Freelancer, and more, so you can find the best projects in one place.
*   **Intelligent Filtering:** Zero in on the perfect projects with powerful, easy-to-use filters for technology, budget, and more.
*   **Effortless Bid Management:** Track your bids from start to finish. Know the status of every application, from "To Bid" to "Won."
*   **Powerful Analytics:** Gain insights into your bidding strategy. Understand your win/loss rate, identify your most successful platforms, and optimize your approach for better results.
*   **Modern, User-Friendly Interface:** A clean, intuitive, and responsive design that works beautifully on any device.

## ğŸ‘¤ Who is BidMaster For?

*   **Freelance Developers & Agencies:** Spend less time searching and more time bidding on high-quality projects.
*   **Project Managers:** Keep all your bids organized and your team on the same page.
*   **Business Development Professionals:** Supercharge your lead generation and close more deals.

## ğŸš€ Get Started in Minutes

1.  **Sign Up:** Create your free BidMaster account.
2.  **Set Your Preferences:** Tell us what kinds of projects you're looking for.
3.  **Discover & Bid:** Start exploring opportunities and winning new business!

Ready to take your freelancing career to the next level?

[**Visit BidMaster Today!**](https://bidmaster-hub.vercel.app/)

---

## ğŸ› ï¸ Developer Documentation

### Quick Start

```bash
# Clone the repository
git clone <repository-url>
cd bidmaster

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env.local
# Edit .env.local with your Supabase credentials

# Run development server
npm run dev

# Open http://localhost:3000
```

### ğŸ§ª Testing & Quality Assurance

#### Crawler Testing Suite

**Basic Validation:**
```bash
npm run test:crawlers          # Quick infrastructure checks
```

**Advanced Testing:**
```bash
npm run test:crawlers:advanced # Comprehensive test suite
```

**Performance Benchmarking:**
```bash
npm run benchmark:crawlers     # Performance analysis
```

**Full Validation:**
```bash
npm run scrape:validate        # Run all tests
```

#### Test Results Overview
- âœ… **Infrastructure**: File existence, module exports, API routes
- âœ… **Configuration**: Platform configs, selectors, rate limiting  
- âœ… **Data Validation**: Interface completeness, mock data, saving logic
- âœ… **API Endpoints**: Route accessibility, request handling
- âœ… **Performance**: Optimization features, concurrent operations
- âœ… **Security**: Environment setup, anti-bot measures, input validation
- âœ… **Integration**: Database connectivity, frontend integration

### ğŸš€ Production Scripts

```bash
npm run build                  # Build for production
npm run start                  # Start production server
npm run lint                   # Code linting
npm run type-check            # TypeScript validation
```

### ğŸ“Š Web Scraping Features

#### Platform Support
- **Upwork**: Job discovery and project scraping
- **Freelancer**: Project listings and bid tracking
- **Extensible**: Easy to add new platforms

#### Scraping Capabilities
- **Real-time Project Discovery**: Automated job searching
- **Mock Data Fallback**: Reliable development experience
- **Rate Limiting**: Respectful scraping practices
- **Error Handling**: Robust error recovery
- **Performance Monitoring**: Built-in benchmarking

#### API Endpoints
```bash
# Test scraping functionality
curl -X POST http://localhost:3000/api/scrape \
  -H 'Content-Type: application/json' \
  -d '{"searchTerm":"react developer","maxResults":3}'

# Get scraping statistics
curl http://localhost:3000/api/scrape/stats

# Cleanup old data
curl -X POST http://localhost:3000/api/scrape/cleanup \
  -H 'Content-Type: application/json' \
  -d '{"dryRun":true}'
```

### ğŸ—ï¸ Architecture & Technology Stack

#### Frontend
- **Next.js 15.3.5**: React framework with App Router
- **TypeScript**: Type-safe development
- **TailwindCSS**: Utility-first styling
- **shadcn/ui**: Modern component library
- **React Query**: State management and data fetching

#### Backend & Database
- **Supabase**: PostgreSQL database and authentication
- **Prisma**: Database ORM (optional integration)
- **Row Level Security**: Data protection

#### Web Scraping
- **Puppeteer**: Headless browser automation
- **Rate Limiting**: Respectful scraping practices
- **Mock Data**: Development-friendly fallbacks
- **Performance Monitoring**: Built-in analytics

#### Testing & Quality
- **ESLint**: Code linting and formatting
- **TypeScript**: Static type checking
- **Custom Test Suite**: Crawler validation framework
- **Performance Benchmarking**: Automated performance analysis

### ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                  # Next.js App Router pages
â”‚   â”‚   â”œâ”€â”€ api/             # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ scrape/      # Scraping endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ projects/    # Project management
â”‚   â”‚   â”‚   â””â”€â”€ bids/        # Bid tracking
â”‚   â”‚   â”œâ”€â”€ auth/            # Authentication pages
â”‚   â”‚   â”œâ”€â”€ projects/        # Project discovery
â”‚   â”‚   â”œâ”€â”€ bids/            # Bid management
â”‚   â”‚   â”œâ”€â”€ analytics/       # Performance analytics
â”‚   â”‚   â””â”€â”€ settings/        # User preferences
â”‚   â”œâ”€â”€ components/          # Reusable UI components
â”‚   â”œâ”€â”€ lib/                 # Utility libraries
â”‚   â”‚   â”œâ”€â”€ scraper.ts       # Web scraping logic
â”‚   â”‚   â”œâ”€â”€ supabase.ts      # Database client
â”‚   â”‚   â””â”€â”€ api.ts           # API utilities
â”‚   â”œâ”€â”€ hooks/               # Custom React hooks
â”‚   â””â”€â”€ types/               # TypeScript definitions
â”œâ”€â”€ database/                # Database schemas
â”œâ”€â”€ scripts/                 # Development and test scripts
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ tasks/                   # Project task tracking
```

### ğŸ”§ Configuration

#### Environment Variables
```bash
# Required for production
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# Optional for development
DATABASE_URL=your_database_url
```

#### Scraping Configuration
- Platform-specific selectors in `src/lib/scraper.ts`
- Rate limiting settings
- Mock data toggle for development
- Performance monitoring settings

### ğŸ“š Documentation

- ğŸ“– **[User Guide](USER_GUIDE.md)**: Step-by-step user documentation
- ğŸ§ª **[Crawler Enhancement Summary](CRAWLER_ENHANCEMENT_SUMMARY.md)**: Technical implementation details
- ğŸ“‹ **[Task Tracking](tasks/tasks.json)**: Development progress and roadmap

### ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Run tests: `npm run scrape:validate`
4. Commit changes: `git commit -m 'Add amazing feature'`
5. Push to branch: `git push origin feature/amazing-feature`
6. Open a Pull Request

### ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

### ğŸ†˜ Support

- ğŸ“§ **Email**: support@bidmaster.com
- ğŸ› **Issues**: GitHub Issues
- ğŸ’¬ **Discussions**: GitHub Discussions

---

**Built with â¤ï¸ for the freelance community**